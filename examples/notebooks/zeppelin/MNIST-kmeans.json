{"paragraphs":[{"text":"%md\n\nThis notebook will show how to classify handwritten digits using the KMeans clustering algorithm through the SageMakerSparkSDK.\n\nWe will train on Amazon SageMaker using the KMeans Clustering on the MNIST dataset, host the trained model on Amazon SageMaker, and then make predictions against that hosted model.\n\nFirst, we load the MNIST dataset into a Spark Dataframe, which dataset is available in LibSVM format at s3://awsai-sparksdk-dataset/mnist_libsvm/","dateUpdated":"2017-11-21T21:30:26+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>This notebook will show how to classify handwritten digits using the KMeans clustering algorithm through the SageMakerSparkSDK.</p>\n<p>We will train on Amazon SageMaker using the KMeans Clustering on the MNIST dataset, host the trained model on Amazon SageMaker, and then make predictions against that hosted model.</p>\n<p>First, we load the MNIST dataset into a Spark Dataframe, which dataset is available in LibSVM format at s3://awsai-sparksdk-dataset/mnist_libsvm/</p>\n"}]},"apps":[],"jobName":"paragraph_1511299826972_-757651706","id":"20171027-200858_635792860","dateCreated":"2017-11-21T21:30:26+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:262"},{"text":"%spark\nimport org.apache.spark.sql.SparkSession\n\nval spark = SparkSession.builder.getOrCreate\n\nval region = \"us-east-1\"\nval trainingData = spark.read.format(\"libsvm\")\n  .option(\"numFeatures\", \"784\")\n  .load(s\"s3://sagemaker-sample-data-$region/spark/mnist/train/\")\n\nval testData = spark.read.format(\"libsvm\")\n  .option(\"numFeatures\", \"784\")\n  .load(s\"s3://sagemaker-sample-data-$region/spark/mnist/test/\")","dateUpdated":"2017-11-21T21:30:52+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511299826972_-757651706","id":"20171027-194154_1485869505","dateCreated":"2017-11-21T21:30:26+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:263"},{"text":"%md\n\nIn order to train and make inferences our input DataFrame must have a column of Doubles (named \"label\" by default) and a column of Vectors of Doubles (named \"features\" by default).\n\nSpark's LibSVM DataFrameReader loads a DataFrame already suitable for training and inference.","dateUpdated":"2017-11-21T21:30:26+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>In order to train and make inferences our input DataFrame must have a column of Doubles (named &ldquo;label&rdquo; by default) and a column of Vectors of Doubles (named &ldquo;features&rdquo; by default).</p>\n<p>Spark's LibSVM DataFrameReader loads a DataFrame already suitable for training and inference.</p>\n"}]},"apps":[],"jobName":"paragraph_1511299826973_-758036455","id":"20171027-201017_1626571936","dateCreated":"2017-11-21T21:30:26+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:264"},{"text":"%spark\n\ntrainingData.show","dateUpdated":"2017-11-21T21:30:55+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511299826973_-758036455","id":"20171027-194234_1638663044","dateCreated":"2017-11-21T21:30:26+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:265"},{"text":"%md\n\nNow we create a KMeansSageMakerEstimator, which uses the KMeans Amazon SageMaker Algorithm to train on our input data, and uses the KMeans Amazon SageMaker model image to host our model.\n\nCalling fit() on this estimator will train our model on Amazon SageMaker, and then create an Amazon SageMaker Endpoint to host our model.\n\nWe can then use the SageMakerModel returned by this call to fit() to transform Dataframes using our hosted model.","dateUpdated":"2017-11-21T21:30:26+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Now we create a KMeansSageMakerEstimator, which uses the KMeans Amazon SageMaker Algorithm to train on our input data, and uses the KMeans Amazon SageMaker model image to host our model.</p>\n<p>Calling fit() on this estimator will train our model on Amazon SageMaker, and then create an Amazon SageMaker Endpoint to host our model.</p>\n<p>We can then use the SageMakerModel returned by this call to fit() to transform Dataframes using our hosted model.</p>\n"}]},"apps":[],"jobName":"paragraph_1511299826974_-756882209","id":"20171027-201222_716997275","dateCreated":"2017-11-21T21:30:26+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:266"},{"text":"%spark\nimport java.util.UUID\n\nimport com.amazonaws.services.sagemaker.sparksdk.KMeansSageMakerEstimator\nimport com.amazonaws.services.sagemaker.sparksdk.IAMRole\n\nval uuid: String = UUID.randomUUID.toString.split(\"-\")(0)\n\nval roleArn = \"arn:aws:iam::142577830533:role/SageMakerRole\"\n\n// kMeansSageMakerEstimator\nval kMeansSageMakerEstimator = new KMeansSageMakerEstimator(\n  sagemakerRole = IAMRole(roleArn),\n  trainingInstanceType = \"ml.p2.xlarge\",\n  trainingInstanceCount = 2,\n  endpointInstanceType = \"ml.c4.xlarge\",\n  endpointInitialInstanceCount = 1)\n  .setK(10).setFeatureDim(784)\n\n// train\nval trainedKMeansModel = kMeansSageMakerEstimator.fit(trainingData)","dateUpdated":"2017-11-21T21:30:58+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511299826975_-757266957","id":"20171027-194247_300882492","dateCreated":"2017-11-21T21:30:26+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:267"},{"text":"%md\n\nNow we transform our DataFrame.\nTo do this, we serialize each row's \"features\" Vector of Doubles into a Protobuf format for inference against the Amazon SageMaker Endpoint. We deserialize the Protobuf responses back into our DataFrame:","dateUpdated":"2017-11-21T21:30:26+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>Now we transform our DataFrame.\n<br  />To do this, we serialize each row's &ldquo;features&rdquo; Vector of Doubles into a Protobuf format for inference against the Amazon SageMaker Endpoint. We deserialize the Protobuf responses back into our DataFrame:</p>\n"}]},"apps":[],"jobName":"paragraph_1511299826975_-757266957","id":"20171027-201535_1579479","dateCreated":"2017-11-21T21:30:26+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:268"},{"text":"%spark\n// inference. For simplicity, we're just transforming our training data.\nval transformedData = trainedKMeansModel.transform(testData)\n// This is used in the SQL queries below\ntransformedData.createOrReplaceTempView(\"transformedData\")\ntransformedData.show","dateUpdated":"2017-11-21T21:31:08+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511299826976_-771502667","id":"20171027-195712_1429851239","dateCreated":"2017-11-21T21:30:26+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:269"},{"text":"%pyspark\n\nfrom pyspark.sql.types import DoubleType\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# helper function to display a digit\ndef show_digit(img, caption='', xlabel='', subplot=None):\n    if subplot==None:\n        _,(subplot)=plt.subplots(1,1)\n    imgr=img.reshape((28,28))\n    subplot.axes.get_xaxis().set_ticks([])\n    subplot.axes.get_yaxis().set_ticks([])\n    plt.title(caption)\n    plt.xlabel(xlabel)\n    subplot.imshow(imgr, cmap='gray')\n\ntransformedDataFrame = sqlContext.table(\"transformedData\").cache()\n\nimages = np.array(transformedDataFrame.select(\"features\").cache().take(100))\nclusters = transformedDataFrame.select(\"closest_cluster\").cache().take(100)\n\nfor cluster in range(10):\n    print('\\n\\n\\nCluster {}:'.format(int(cluster)))\n    digits = [ img for l, img in zip(clusters, images) if int(l.closest_cluster) == cluster ]\n    height=((len(digits)-1)//5)+1\n    width=5\n    plt.rcParams[\"figure.figsize\"] = (width,height)\n    _, subplots = plt.subplots(height, width)\n    subplots=np.ndarray.flatten(subplots)\n    for subplot, image in zip(subplots, digits):\n        show_digit(image, subplot=subplot)\n    for subplot in subplots[len(digits):]:\n        subplot.axis('off')\n\n    plt.show()","dateUpdated":"2017-11-21T21:30:26+0000","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"language":"python","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511299826976_-771502667","id":"20171030-210420_1123789562","dateCreated":"2017-11-21T21:30:26+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:270"},{"text":"%md\n\nThe KMeansSageMakerEstimator is just an SageMakerEstimator with certain default values. You can use the Amazon SageMaker Spark SDK with algorithm and hosting images you have authored by directly instantiating (or extending) the SageMakerEstimator.\n\nIn addition to the training and hosting parameters, you need to pass in your training and model images, training hyperparameters, and a RequestRowSerializer and a ResponseRowDeserializer.\n\nThe RequestRowSerializer is what the SageMakerModel uses upon a call to transform() to serialize Spark Dataframe rows into byte arrays to send to the Amazon SageMaker Endpoint for transformation.\nThe ResponseRowDeserializer is what the SageMakerModel uses upon a call to transform() to the Amazon SageMaker Endpoint's response from a byte array into Rows of a DataFrame.\n\nThe ProtobufRequestRowSerializer serializes Spark Rows into a protobuf format that the KMeans, PCA, and LinearLearner algorithms can consume for inference.\nThe KMeansProtobufResponseRowDeserializer deserializes the protobuf response from the KMeans model image.","dateUpdated":"2017-11-21T21:30:26+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<p>The KMeansSageMakerEstimator is just an SageMakerEstimator with certain default values. You can use the Amazon SageMaker Spark SDK with algorithm and hosting images you have authored by directly instantiating (or extending) the SageMakerEstimator.</p>\n<p>In addition to the training and hosting parameters, you need to pass in your training and model images, training hyperparameters, and a RequestRowSerializer and a ResponseRowDeserializer.</p>\n<p>The RequestRowSerializer is what the SageMakerModel uses upon a call to transform() to serialize Spark Dataframe rows into byte arrays to send to the Amazon SageMaker Endpoint for transformation.\n<br  />The ResponseRowDeserializer is what the SageMakerModel uses upon a call to transform() to the Amazon SageMaker Endpoint's response from a byte array into Rows of a DataFrame.</p>\n<p>The ProtobufRequestRowSerializer serializes Spark Rows into a protobuf format that the KMeans, PCA, and LinearLearner algorithms can consume for inference.\n<br  />The KMeansProtobufResponseRowDeserializer deserializes the protobuf response from the KMeans model image.</p>\n"}]},"apps":[],"jobName":"paragraph_1511299826977_-771887416","id":"20171027-194607_461915056","dateCreated":"2017-11-21T21:30:26+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:271"},{"text":"%spark\n\nimport com.amazonaws.services.sagemaker.sparksdk.SageMakerEstimator\nimport com.amazonaws.services.sagemaker.sparksdk.transformation.serializers.ProtobufRequestRowSerializer\nimport com.amazonaws.services.sagemaker.sparksdk.transformation.deserializers.KMeansProtobufResponseRowDeserializer\n\nval kmeans = new SageMakerEstimator(\n      trainingImage = \"382416733822.dkr.ecr.us-east-1.amazonaws.com/kmeans:1\",\n      modelImage = \"382416733822.dkr.ecr.us-east-1.amazonaws.com/kmeans:1\",\n      requestRowSerializer = new ProtobufRequestRowSerializer(),\n      responseRowDeserializer = new KMeansProtobufResponseRowDeserializer(),\n      hyperParameters = Map(\"k\" -> \"10\", \"feature_dim\" -> \"784\"),\n      sagemakerRole = IAMRole(roleArn),\n      trainingInstanceType = \"ml.p2.xlarge\",\n      trainingInstanceCount = 2,\n      endpointInstanceType = \"ml.c4.xlarge\",\n      endpointInitialInstanceCount = 1)","dateUpdated":"2017-11-22T00:30:51+0000","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"language":"scala","editOnDblClick":false}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511299826978_-770733169","id":"20171027-195104_2011298555","dateCreated":"2017-11-21T21:30:26+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:272"},{"text":"%md\n\n\n","dateUpdated":"2017-11-21T21:30:26+0000","config":{"tableHide":false,"editorSetting":{"language":"markdown","editOnDblClick":true},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1511299826978_-770733169","id":"20171027-195210_612973143","dateCreated":"2017-11-21T21:30:26+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:273"}],"name":"MNIST-Libsvm","id":"2D1ZVYZ5E","angularObjects":{"2BRWU4WXC:shared_process":[],"2AM1YV5CU:shared_process":[],"2AJXGMUUJ:shared_process":[],"2ANGGHHMQ:shared_process":[],"2AKK3QQXU:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}